{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGEDIR='images'\n",
    "PROFILES='real'\n",
    "\n",
    "iurlrx = re.compile('.* background-image: url\\(([^\\)]+)\\)')\n",
    "\n",
    "remap = {'I am' : 'gender',\n",
    "         'Age' : 'age',\n",
    "         'City' : 'location',\n",
    "         'Marital status' : 'status',\n",
    "         'Username' : 'username',\n",
    "         'Ethnicity' : 'ethnicity',\n",
    "         'Occupation' : 'occupation',\n",
    "         'About me' : 'description',\n",
    "         'My match\\'s age' : 'match_age',\n",
    "         'Children' : 'children',\n",
    "         'Sexual Orientation' : 'orientation',\n",
    "         'Religion' : 'religion',\n",
    "         'Do you smoke' : 'smoking',\n",
    "         'Do you drink' : 'drinking',\n",
    "         'Here for' : 'intent'}\n",
    "\n",
    "def save_image(url):\n",
    "    \"\"\" Take a URL, generate a unique filename, save \n",
    "        the image to said file and return the filename.\"\"\"\n",
    "    ext = url.split('.')[-1]\n",
    "    filename = IMAGEDIR+os.sep+hashlib.md5(url.encode('utf-8')).hexdigest()+'.'+ext\n",
    "    if os.path.exists(filename):\n",
    "        return filename\n",
    "    try:\n",
    "        content = urlopen(url).read()\n",
    "        f = open(filename,'wb') \n",
    "        f.write(content)\n",
    "        f.close()\n",
    "    except e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return filename \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate a list of urls within the scrape profiles \n",
    "- then go into each url to get the profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_profile(inhandle):\n",
    "  \"\"\"Scrape an input scamdiggers page for the profile content\n",
    "  of the scammer. \"\"\"\n",
    "  #Read file\n",
    "  html = inhandle.read()\n",
    "  soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "  #pfnode = soup.find('div', {'class':'profile-BASE_CMP_UserViewWidget'})\n",
    "  #avnode = soup.find(id='avatar_console_image')\n",
    "  target_divs = soup.find_all('div', class_='ow_user_list_item clearfix ow_item_set3')\n",
    "  # Initialize an empty list to store unique links\n",
    "  unique_links = []\n",
    "\n",
    "  # Iterate over each div element\n",
    "  for div in target_divs:\n",
    "      # Find all 'a' tags with 'href' attribute and extract the 'href' value\n",
    "    links = [a['href'] for a in div.find_all('a', href=True)]\n",
    "    # Add links to the unique_links list\n",
    "    unique_links.extend(links)\n",
    "\n",
    "  # Get unique links\n",
    "  unique_links = list(set(unique_links))\n",
    "\n",
    "\n",
    "  for link in unique_links:\n",
    "    urltemp = urlopen(link,context=ssl._create_unverified_context())\n",
    "    htmltemp = urltemp.read()\n",
    "    souptemp = BeautifulSoup(htmltemp, 'html.parser')\n",
    "    #Pull the provided profile data out.\n",
    "    rows = souptemp.findAll('tr')\n",
    "    labels = {}\n",
    "    for row in rows:\n",
    "      lab = row.find('td',{'class':'ow_label'})\n",
    "      val = row.find('td',{'class':'ow_value'})\n",
    "      if lab:\n",
    "        labels[lab.get_text()] = val.get_text().strip()\n",
    "\n",
    "    profile = {}\n",
    "\n",
    "    #Populate our own profile structure.\n",
    "    for lab in remap:\n",
    "      if lab in labels:\n",
    "        profile[remap[lab]] = labels[lab]\n",
    "      else:\n",
    "        profile[remap[lab]] = \"-\"\n",
    "  \n",
    "    #Tweak for consistency.\n",
    "    profile['gender'] = profile['gender'].lower()\n",
    "  \n",
    "    #Extract avatar image\n",
    "    #img = iurlrx.match(avnode.attrs['style']).group(1)\n",
    "    #profile['images'] = [save_image(img)]\n",
    "\n",
    "    #Save output\n",
    "    print(profile)\n",
    "    outfile=outfile=profile['username']+'.json'\n",
    "    json.dump(profile, open(outfile,'w'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_profiles(inhandle):\n",
    "  \"\"\" Extract all the profile page links from\n",
    "  this index page. \"\"\"\n",
    "  html = inhandle.read()\n",
    "  soup = BeautifulSoup(html, 'html.parser')\n",
    "  \n",
    "  urls = [ node.find('a')['href'] for node in soup.findAll('div',  {'class':'ow_user_list_data'})]\n",
    "  return urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin URL harvesting.\n",
      "<http.client.HTTPResponse object at 0x10c40c340>\n",
      "{'gender': 'male', 'age': '25 y.o.', 'location': 'Denver, CO, USA', 'status': 'single', 'username': 'Jesus001', 'ethnicity': 'hispanic', 'occupation': '-', 'description': '-', 'match_age': 'from 19 to 35', 'children': 'no children', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'light smoker', 'drinking': 'occasional drinker', 'intent': 'Fun, Friendship, Romance, Serious Relationship, Marriage'}\n",
      "{'gender': 'male', 'age': '36 y.o.', 'location': 'Lima, Peru', 'status': 'single', 'username': 'EddieFleurty', 'ethnicity': 'white', 'occupation': '-', 'description': '-', 'match_age': 'from 19 to 55', 'children': 'no children', 'orientation': 'Straight', 'religion': 'Other', 'smoking': 'non-smoker', 'drinking': 'occasional drinker', 'intent': 'Fun, Friendship, Romance, Serious Relationship, Marriage'}\n",
      "{'gender': 'female', 'age': '33 y.o.', 'location': 'Melbourne VIC, Australia', 'status': 'in relationship', 'username': 'Naughtynnice', 'ethnicity': 'white', 'occupation': 'Dancer', 'description': 'Open minded looking to spice up my life', 'match_age': 'from 44 to 47', 'children': 'want children', 'orientation': 'Straight', 'religion': 'Other', 'smoking': 'light smoker', 'drinking': 'social drinker', 'intent': 'Fun, Friendship, Romance, Serious Relationship'}\n",
      "{'gender': 'male', 'age': '49 y.o.', 'location': 'Hillsboro, TX 76645, USA', 'status': 'divorced', 'username': 'Ph1', 'ethnicity': 'black', 'occupation': 'Business owner', 'description': '-', 'match_age': 'from 19 to 86', 'children': 'no children', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'non-smoker', 'drinking': 'never', 'intent': 'Fun, Friendship, Romance, Serious Relationship'}\n",
      "{'gender': 'female', 'age': '50 y.o.', 'location': 'Reșița, Romania', 'status': 'single', 'username': 'Alexa73', 'ethnicity': 'white', 'occupation': 'Economist', 'description': '-', 'match_age': 'from 50 to 56', 'children': 'no children', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'non-smoker', 'drinking': 'never', 'intent': 'Friendship, Romance, Serious Relationship, Marriage'}\n",
      "{'gender': 'female', 'age': '30 y.o.', 'location': 'Durban, South Africa', 'status': 'single', 'username': 'Lelo', 'ethnicity': 'black', 'occupation': 'Accounts', 'description': '-', 'match_age': 'from 30 to 40', 'children': '1-2 living elsewhere', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'non-smoker', 'drinking': 'never', 'intent': 'Romance, Serious Relationship'}\n",
      "{'gender': 'male', 'age': '52 y.o.', 'location': '50050 Cerreto Guidi, Metropolitan City of Florence, Italy', 'status': 'separated', 'username': 'Danielec', 'ethnicity': 'white', 'occupation': 'Libero professionista', 'description': '-', 'match_age': 'from 38 to 45', 'children': '1-2 living elsewhere', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'non-smoker', 'drinking': 'occasional drinker', 'intent': 'Serious Relationship'}\n",
      "{'gender': 'male', 'age': '31 y.o.', 'location': 'Waterbury, CT, USA', 'status': 'single', 'username': 'Luisjose32', 'ethnicity': 'hispanic', 'occupation': '-', 'description': '-', 'match_age': 'from 19 to 86', 'children': 'want children', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'non-smoker', 'drinking': 'occasional drinker', 'intent': 'Fun, Romance'}\n",
      "{'gender': 'male', 'age': '42 y.o.', 'location': 'Madrid, España', 'status': 'single', 'username': 'Tony33', 'ethnicity': 'hispanic', 'occupation': '-', 'description': '-', 'match_age': 'from 30 to 86', 'children': 'no children', 'orientation': 'Straight', 'religion': 'Atheist', 'smoking': 'non-smoker', 'drinking': 'never', 'intent': 'Serious Relationship, Marriage'}\n",
      "{'gender': 'male', 'age': '33 y.o.', 'location': 'Zulia, Venezuela', 'status': 'single', 'username': 'Portillo2121', 'ethnicity': 'hispanic', 'occupation': 'Independiente', 'description': '-', 'match_age': 'from 30 to 55', 'children': 'no children', 'orientation': 'Straight', 'religion': 'Christian', 'smoking': 'non-smoker', 'drinking': 'social drinker', 'intent': 'Friendship, Romance, Serious Relationship, Marriage'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHarvesting complete. \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m URLs to scrape.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(urls)))\n\u001b[1;32m     26\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mScraping complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m scrape()\n",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m, in \u001b[0;36mscrape\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   urlhandle \u001b[39m=\u001b[39m urlopen(url)\n\u001b[1;32m     14\u001b[0m   \u001b[39mprint\u001b[39m(urlhandle)\n\u001b[0;32m---> 15\u001b[0m   scrape_profile(urlhandle)\n\u001b[1;32m     17\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     18\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mException when handling \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(url))\n",
      "Cell \u001b[0;32mIn[30], line 27\u001b[0m, in \u001b[0;36mscrape_profile\u001b[0;34m(inhandle)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m unique_links:\n\u001b[1;32m     26\u001b[0m   urltemp \u001b[39m=\u001b[39m urlopen(link,context\u001b[39m=\u001b[39mssl\u001b[39m.\u001b[39m_create_unverified_context())\n\u001b[0;32m---> 27\u001b[0m   htmltemp \u001b[39m=\u001b[39m urltemp\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m     28\u001b[0m   souptemp \u001b[39m=\u001b[39m BeautifulSoup(htmltemp, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m   \u001b[39m#Pull the provided profile data out.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_chunked(amt)\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m         \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:583\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m         chunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_chunk_left()\n\u001b[1;32m    584\u001b[0m         \u001b[39mif\u001b[39;00m chunk_left \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:566\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_safe_read(\u001b[39m2\u001b[39m)  \u001b[39m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     chunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_next_chunk_size()\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:526\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_next_chunk_size\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[39m# Read the next chunk size from the file\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    528\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mchunk size\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scrape():\n",
    "  \"\"\" Harvest profiles from every third page from the site. \"\"\"\n",
    "  urls = []\n",
    "  urlstr=\"http://datingnmore.com/site/users/latest?page={}\" \n",
    "\n",
    "  print(\"Begin URL harvesting.\")\n",
    "\n",
    "  #For every third page (sample size calculated to finish overnight). \n",
    "  # edit the number of pages u want \n",
    "  for i in range(1,10,3):\n",
    "    url = urlstr.format(i)\n",
    "    jitter = random.choice([0,1])\n",
    "    try:\n",
    "      urlhandle = urlopen(url)\n",
    "      print(urlhandle)\n",
    "      scrape_profile(urlhandle)\n",
    "      \n",
    "    except Exception as e:\n",
    "      print(\"Exception when handling {}\".format(url))\n",
    "      print(e)\n",
    "      break\n",
    "\n",
    "  print(\"Harvesting complete. {} URLs to scrape.\".format(len(urls)))\n",
    "      \n",
    " \n",
    " \n",
    "  print(\"Scraping complete.\")\n",
    "\n",
    "\n",
    "scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
